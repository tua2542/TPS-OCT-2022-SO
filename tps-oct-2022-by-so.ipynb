{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Author \n\nSakdipat Ontoum\n\nhttps://www.linkedin.com/in/sakdipat-ontoum-256bb0209/","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n![](https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/1.jpeg)\n\nRocket League is a video game about soccer. Each of the two teams has up to eight players who use rocket-powered vehicles to strike a ball into their opponent's goal and collect points during the course of a match. In addition, Tabular Playground Series - Oct 2022 give challenges this video game to everyone to predict the probability of each team scoring within the next 10 seconds of the game.\n\n# Objective\n\nTo create model that can predict the probability of each team scoring within the next 10 seconds of the game.\n\n# Approach\n\nMaybe Logistic Regression for predicting, and Principal component analysis (PCA) / t-distributed stochastic neighbor embedding (t-SNE) for reducing dimention of data. I will try to play around of them as possible.\n\n\n# Dataset\n\nI used the data from [Tabular Playground Series - Oct 2022](https://www.kaggle.com/competitions/tabular-playground-series-oct-2022/). There is made up of sequences of snapshots of the current state of a Rocket League match, including the position and velocity of all players and the ball.\n\n# Performance Measure\n\nI will use the formula that [Tabular Playground Series - Oct 2022](https://www.kaggle.com/competitions/tabular-playground-series-oct-2022/) given. the log loss formula to measure the performace measure as below here.\n\n$$\\large score = -\\frac{1}{2}\\sum_{m=1}^{M}[y_{i,m}\\log (\\hat{y_{i, m}})+(1-y_{i, m})\\log (1-\\hat{y_{i, m}})] $$\n\nwhere:\n\n* $N$ is the number of id observations in the test data\n* $M$ is the number of scored targets (here , one for each team)\n* $\\hat{y_{i,m}}$ is the predicted scoring probability of team  (Team A or Team B in the dataset)\n* ${y_{i,m}}$ is the ground truth for team , 1 for a goal within 10 seconds, 0 otherwise\n* $\\log ()$ is the natural (base e) logarithm\n\n \nNote: the actual submitted predicted probabilities are replaced with $\\max (\\min (p,1-10^{-15}), 10^{-15})$.  A smaller log loss is better.","metadata":{}},{"cell_type":"markdown","source":"# 1. Configurations\n\n## 1.1 Install and Imports Library\n\nAll the install library go here. ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\nfrom pathlib import Path\n\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.model_selection import KFold, cross_val_score, cross_validate\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Global Settings\n\nThere variables will be used across the project.","metadata":{}},{"cell_type":"code","source":"#PCA Setting\nAMOUNT_COMPONENTS = 10\n\n#Model Setting\nparams = {\n      'objective': 'binary',\n      'metric': 'logloss',\n      'num_iterations': 500\n     }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Data Importation","metadata":{}},{"cell_type":"code","source":"#check overall file in directories\nrun_this = False\n\nif run_this:\n    for dirname, _ ,filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see that on the Tabular Playground the Series on Oct 2022, there has 10 training, 1 testing, and 1 sample_submission files.","metadata":{}},{"cell_type":"code","source":"%%time\n#Convert csv file to parquet feather since there are big data.\nrun_this = False\n\nif run_this:\n#training set\n    for index in range(10):\n        train_dataframe = pd.read_csv(f'../input/tabular-playground-series-oct-2022/train_{index}.csv')\n        train_dataframe.to_parquet(f'train_{index}_parquet.gzip', compression='gzip')\n        print('Done with train file', index)\n\n#testing set\n    test_dataframe = pd.read_csv(f'../input/tabular-playground-series-oct-2022/test.csv')\n    test_dataframe.to_parquet(f'test.parquet.gzip', compression='gzip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\n    'ball_pos_x', 'ball_pos_y','ball_pos_z', 'ball_vel_x', 'ball_vel_y', 'ball_vel_z', \n    'p0_pos_x', 'p0_pos_y', 'p0_pos_z', 'p0_vel_x', 'p0_vel_y', 'p0_vel_z', 'p0_boost', 'p0_na',\n    'p1_pos_x', 'p1_pos_y', 'p1_pos_z', 'p1_vel_x', 'p1_vel_y', 'p1_vel_z', 'p1_boost', 'p1_na',\n    'p2_pos_x', 'p2_pos_y', 'p2_pos_z', 'p2_vel_x', 'p2_vel_y', 'p2_vel_z', 'p2_boost', 'p2_na',\n    'p3_pos_x', 'p3_pos_y', 'p3_pos_z', 'p3_vel_x', 'p3_vel_y', 'p3_vel_z', 'p3_boost', 'p3_na',\n    'p4_pos_x', 'p4_pos_y', 'p4_pos_z', 'p4_vel_x', 'p4_vel_y', 'p4_vel_z', 'p4_boost', 'p4_na',\n    'p5_pos_x', 'p5_pos_y', 'p5_pos_z', 'p5_vel_x', 'p5_vel_y', 'p5_vel_z', 'p5_boost', 'p5_na',\n    'boost0_timer', 'boost1_timer', 'boost2_timer', 'boost3_timer',\n    'boost4_timer', 'boost5_timer']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [\n    'team_A_scoring_within_10sec',\n    'team_B_scoring_within_10sec']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = False\ninput_path = Path('../input/fast-loading-high-compression-with-feather/feather_data')\n\ndef fe(x):\n#     # indicators for respawns...\n#     x['p0_na'] = x['p0_pos_x'].isna().astype('int8')\n#     x['p1_na'] = x['p1_pos_x'].isna().astype('int8')\n#     x['p2_na'] = x['p2_pos_x'].isna().astype('int8')\n#     x['p3_na'] = x['p3_pos_x'].isna().astype('int8')\n#     x['p4_na'] = x['p4_pos_x'].isna().astype('int8')\n#     x['p5_na'] = x['p5_pos_x'].isna().astype('int8')\n    for feature in features:\n        if feature.endswith('_na'):\n            continue\n        if feature.endswith('_x'):\n            x[feature] = (x[feature] / 82).fillna(0).astype('float16')\n        if feature.endswith('_y'):\n            x[feature] = (x[feature] / 120).fillna(0).astype('float16')\n        if feature.endswith('_z'):\n            x[feature] = (x[feature] / 40).fillna(0).astype('float16')\n        if feature.endswith('_boost'):\n            x[feature] = (x[feature] / 100).fillna(0).astype('float16')\n        if feature.endswith('_timer'):\n            x[feature] = (-x[feature] / 100).astype('float16')\n    return x\n\ndef read_train():\n    dfs = []\n    for i in range(10):\n        dfs.append(fe(pd.read_feather(input_path / f'train_{i}_compressed.ftr')))\n    result = pd.concat(dfs)\n    if DEBUG:\n        result = result.sample(frac=0.05)\n    return result\n\ndef read_test():\n    return fe(pd.read_feather(input_path / 'test_compressed.ftr'))\n\ntrain_dataframe = read_train()\ngc.collect()\ntest_dataframe = read_test()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#Read some of training set and testing set that already converted from .csv to parquet\n# train_dataframe = pd.read_parquet('../input/tps-oct-2022-compressed-parquet-files/train_0.parquet.gzip')\n# train_dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataframe = pd.read_parquet('../input/tps-oct-2022-compressed-parquet-files/test.parquet.gzip')\n# test_dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataframe.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def euclidian_norm(x):\n    return np.linalg.norm(x, axis=1)\n\nvel_groups = {\n    f\"{el}_vel\": [f'{el}_vel_x', f'{el}_vel_y', f'{el}_vel_z']\n    for el in ['ball'] + [f'p{i}' for i in range(6)]\n}\npos_groups = {\n    f\"{el}_pos\": [f'{el}_pos_x', f'{el}_pos_y', f'{el}_pos_z']\n    for el in ['ball'] + [f'p{i}' for i in range(6)]\n}\n\nfor col, vec in vel_groups.items():\n    train_dataframe[col] = euclidian_norm(train_dataframe[vec])\n    test_dataframe[col] = euclidian_norm(test_dataframe[vec])\n    \nfor col, vec in pos_groups.items():\n    train_dataframe[col + \"_ball_dist\"] = euclidian_norm(train_dataframe[vec].values - train_dataframe[pos_groups[\"ball_pos\"]].values)\n    test_dataframe[col + \"_ball_dist\"] = euclidian_norm(test_dataframe[vec].values - test_dataframe[pos_groups[\"ball_pos\"]].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop Columns\nfeatures = list(train_dataframe.columns[3:-24]) + list(train_dataframe.columns[-14:])\ntest_dataframe = test_dataframe[features].copy()\nfeatures.append('team_A_scoring_within_10sec')\nfeatures.append('team_B_scoring_within_10sec')\ntrain_dataframe = train_dataframe[features].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Deal with missing Values\ntrain_dataframe = train_dataframe.dropna()\ntest_dataframe = test_dataframe.fillna(value=test_dataframe.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dimensionally Reduction with PCA\nfeatures = list(train_dataframe.columns)\nlen(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.remove('team_A_scoring_within_10sec')\nfeatures.remove('team_B_scoring_within_10sec')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe = pd.concat([train_dataframe[features], test_dataframe])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number = AMOUNT_COMPONENTS\npca = PCA(n_components=number)\nrotatedData = pca.fit_transform(dataframe)\n\n\nnew_features = []\nfor index in range(number):\n    new_features.append(f'X{index}')\n\nPCA_dataframe = pd.DataFrame(data=rotatedData, columns=new_features)\nPCA_dataframe.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataframe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = PCA_dataframe.iloc[:len(train_dataframe)].copy()\nX_test = PCA_dataframe.iloc[len(train_dataframe):].copy()\n\ny_train_team_A = train_dataframe['team_A_scoring_within_10sec'].copy()\ny_train_team_B = train_dataframe['team_B_scoring_within_10sec'].copy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Creation","metadata":{}},{"cell_type":"code","source":"def cv_score_team_A(model):\n    k_fold = KFold(5, shuffle=True, random_state=0).get_n_splits(X_train.values)\n    scores = cross_val_score(model, X_train.values, y_train_team_A, scoring='neg_log_loss', cv=k_fold)\n    return scores\n\ndef cv_score_team_B(model):\n    k_fold = KFold(5, shuffle=True, random_state=0).get_n_splits(X_train.values)\n    scores = cross_val_score(model, X_train.values, y_train_team_B, scoring='neg_log_loss', cv=k_fold)\n    return scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = lgb.LGBMClassifier(**params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluation ","metadata":{}},{"cell_type":"code","source":"lgbm_score_team_A = cv_score_team_A(lgbm)\nprint(\"LightGBM score of Team A: {:.4f} ({:.4f})\".format(lgbm_score_team_A.mean(), lgbm_score_team_A.std()))\n\nlgbm_score_team_B = cv_score_team_B(lgbm)\nprint(\"LightGBM score of Team B: {:.4f} ({:.4f})\".format(lgbm_score_team_B.mean(), lgbm_score_team_B.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_team_A = lgb.LGBMClassifier(**params)\nlgbm_team_B = lgb.LGBMClassifier(**params)\n\nlgbm_team_A.fit(X_train, y_train_team_A)\nlgbm_team_B.fit(X_train, y_train_team_B)\n\ny_pred_team_A_lgbm = lgbm_team_A.predict_proba(X_test)[:,1]\ny_pred_team_B_lgbm = lgbm_team_B.predict_proba(X_test)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/tabular-playground-series-oct-2022/sample_submission.csv')\nsubmission['team_A_scoring_within_10sec'] = y_pred_team_A_lgbm\nsubmission['team_B_scoring_within_10sec'] = y_pred_team_B_lgbm\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('TBS-submission-10062022-13.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n\nhttps://www.kaggle.com/code/ryanluoli2/a-simple-lightgbm-baseline-with-pca\n\nhttps://www.kaggle.com/code/reymaster/compress-files-parquet-7x-loading-speedup\n\nhttps://www.kaggle.com/datasets/reymaster/tps-oct-2022-compressed-parquet-files\n\nhttps://en.wikipedia.org/wiki/Rocket_League\n\nhttps://www.kaggle.com/competitions/tabular-playground-series-oct-2022/\n\nhttps://www.kaggle.com/code/chazzer/rocket-league-xgboost-feat-engineering-cv/notebook?scriptVersionId=107113700","metadata":{}},{"cell_type":"markdown","source":"# Thank you ðŸ˜†ðŸ˜†\n\n![](https://wallpaperaccess.com/full/5914023.png)","metadata":{}}]}